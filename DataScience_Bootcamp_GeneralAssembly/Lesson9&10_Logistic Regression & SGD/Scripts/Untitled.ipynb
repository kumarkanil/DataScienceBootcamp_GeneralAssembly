{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GA Data Science Q2 2016\n",
    "\n",
    "Code walk-through 9: Logistic regression using scikit-learn\n",
    "\n",
    "* Logistic regression\n",
    "* Confusion matrix and performance metrics\n",
    "* Visualising the effect of covariates\n",
    "* ROC analysis\n",
    "* Cross-validation\n",
    "* Regularisation\n",
    "* Variable (feature) selection\n",
    "* Multiple classes\n",
    "* Stochastic gradient descent\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model as lm, metrics, cross_validation as cv,\\\n",
    "                    grid_search, feature_selection, preprocessing\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original publication: http://dx.doi.org/10.1371/journal.pone.0137524\n",
    "# Dataset: http://dx.doi.org/10.5061/dryad.8jq92\n",
    "\n",
    "#GLIOMA_URL = 'http://datadryad.org/bitstream/handle/10255/dryad.88928/An%20eighteen%20serum%20cytokine%20signature%20for%20discriminating%20glioma%20from%20normal%20healthy%20individuals%20raw%20data.xlsx?sequence=1'\n",
    "\n",
    "glioma = pd.read_excel('../../Data/glioma.xlsx')\n",
    "\n",
    "# Transpose DataFrame so that measurements are in columns\n",
    "glioma = glioma.transpose()\n",
    "\n",
    "# Set first row as column names, then drop it\n",
    "glioma.columns = glioma.iloc[0]\n",
    "glioma.columns.name = ''\n",
    "glioma = glioma.reindex(glioma.index.drop('sample'))\n",
    "\n",
    "# Extract cytokine measurements\n",
    "X = glioma.iloc[:,1:].apply(pd.to_numeric, axis=1)\n",
    "\n",
    "# Apply logarithmic transformation to each measurement\n",
    "X = X.apply(np.log, axis=1)\n",
    "\n",
    "# Dichotomise outcome: GBM versus rest\n",
    "# \n",
    "# DA  = Diffuse Astrocytoma (grade II)\n",
    "# AA  = Anaplastic Astrocytoma (grade III)\n",
    "# GBM = Glioblastoma Multiforme (grade IV)\n",
    "y = glioma.Type == 'GBM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.37697919e+00,   1.22872574e-03,   6.23272054e+08,\n",
       "          7.83649900e-03,   6.48186744e-03,   1.06837220e-02,\n",
       "          3.18226502e+02,   3.08516339e-01,   7.30410085e+00,\n",
       "          6.87681066e-01,   4.00884339e+08,   1.19510649e-05,\n",
       "          1.00557915e-04,   2.97962454e+01,   6.18009567e-03,\n",
       "          9.45412188e-01,   1.24652626e+01,   1.40437431e-04,\n",
       "          1.55636052e+00,   1.39924040e-07,   1.10030198e-04,\n",
       "          9.21699033e+02,   9.59616033e+03,   9.34998370e-13,\n",
       "          5.37224321e+08,   2.08423766e+00,   8.53199271e-04,\n",
       "          1.65402305e+04,   6.20199070e-01,   1.18056695e-01,\n",
       "          1.10666811e+00,   3.51503901e-02,   8.63801257e-01,\n",
       "          3.11742808e-05,   1.60621463e+02,   6.03073280e+02,\n",
       "          5.34416840e-01,   3.46648739e+01,   4.19521122e-03,\n",
       "          2.06086647e+04,   4.92084410e-01,   6.37174356e+05,\n",
       "          6.91357679e-05,   2.69836170e-03,   7.59623292e+00,\n",
       "          9.34721294e-02,   4.21655514e+01,   4.12725152e-02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Logistic regression\n",
    "'''\n",
    "\n",
    "# Fit the model\n",
    "# NOTE: By default, sklearn uses L2 regularisation with parameter C (default = 1)\n",
    "#       This cannot be disabled, but we can set C so big that it has little effect\n",
    "model1 = lm.LogisticRegression(C=1e50)\n",
    "model1.fit(X, y)\n",
    "\n",
    "# Print regression coefficients\n",
    "model1.intercept_\n",
    "model1.coef_\n",
    "\n",
    "# Print odds ratios\n",
    "np.exp(model1.intercept_)\n",
    "np.exp(model1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00        72\n",
      "       True       1.00      1.00      1.00       148\n",
      "\n",
      "avg / total       1.00      1.00      1.00       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Confusion matrix and performance metrics\n",
    "'''\n",
    "\n",
    "# Confusion matrix\n",
    "metrics.confusion_matrix(y, model1.predict(X))\n",
    "\n",
    "# Classification accuracy\n",
    "metrics.accuracy_score(y, model1.predict(X))\n",
    "\n",
    "# Classification report\n",
    "print(metrics.classification_report(y, model1.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None,\n",
       "           cv=sklearn.cross_validation.StratifiedKFold(labels=[False False ..., False False], n_folds=5, shuffle=True, random_state=None),\n",
       "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
       "           max_iter=100, multi_class='ovr', n_jobs=1, penalty='l1',\n",
       "           random_state=None, refit=True, scoring='roc_auc',\n",
       "           solver='liblinear', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat using L1 regularisation\n",
    "kf = cv.StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "model3 = lm.LogisticRegressionCV(Cs=10, cv=kf, penalty='l1', scoring='roc_auc',\\\n",
    "                                 solver='liblinear')\n",
    "model3.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(model3.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.55226531,   0.86593749,  16.64530907,   0.53754231,\n",
       "          0.34493249,   0.71818373,   2.63069061,   1.        ,\n",
       "          1.        ,   0.82378209,  15.00369372,   0.16470895,\n",
       "          0.27128577,   1.32735853,   1.        ,   1.00368011,\n",
       "          1.6632326 ,   0.39535548,   1.03618749,   0.04747917,\n",
       "          0.31093625,   1.11631486,   2.93800302,   0.0243767 ,\n",
       "         22.52417981,   1.        ,   0.47417344,   1.09835051,\n",
       "          1.04346779,   0.56432097,   1.        ,   0.66886254,\n",
       "          0.88352261,   0.51324481,   1.        ,   3.04474703,\n",
       "          1.        ,   1.35338467,   0.48645861,   4.89359098,\n",
       "          0.63193263,   9.66814095,   0.47881418,   0.55898465,\n",
       "          1.40110373,   1.        ,   2.5381217 ,   0.51013131]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(model3.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(model3.Cs_), model3.scores_[1].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.93726721e-01,  -1.43942560e-01,   2.81212844e+00,\n",
       "        -6.20747815e-01,  -1.06440657e+00,  -3.31029851e-01,\n",
       "         9.67246402e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "        -1.93849240e-01,   2.70829642e+00,  -1.80357532e+00,\n",
       "        -1.30458250e+00,   2.83190897e-01,   0.00000000e+00,\n",
       "         3.67335340e-03,   5.08763056e-01,  -9.27969969e-01,\n",
       "         3.55481063e-02,  -3.04746416e+00,  -1.16816736e+00,\n",
       "         1.10032957e-01,   1.07773010e+00,  -3.71412735e+00,\n",
       "         3.11458939e+00,   0.00000000e+00,  -7.46182123e-01,\n",
       "         9.38095141e-02,   4.25495807e-02,  -5.72132100e-01,\n",
       "         0.00000000e+00,  -4.02176718e-01,  -1.23838393e-01,\n",
       "        -6.67002341e-01,   0.00000000e+00,   1.11341782e+00,\n",
       "         0.00000000e+00,   3.02608621e-01,  -7.20603450e-01,\n",
       "         1.58792639e+00,  -4.58972483e-01,   2.26883604e+00,\n",
       "        -7.36442691e-01,  -5.81633273e-01,   3.37260301e-01,\n",
       "         0.00000000e+00,   9.31424319e-01,  -6.73087114e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Variable (feature) selection\n",
    "'''\n",
    "\n",
    "# Select only variables with non-zero coefficient in L1-regularised model\n",
    "idx = np.where(np.abs(model3.coef_[0]) >= 1e-16)[0]\n",
    "\n",
    "# List selected variables\n",
    "X.columns[idx]\n",
    "\n",
    "# Re-fit model using selected variables only\n",
    "X_selected = X.iloc[:,idx]\n",
    "model4 = lm.LogisticRegression(C=model3.C_[0])\n",
    "model4.fit(X_selected, y)\n",
    "\n",
    "# Plot ROC curve\n",
    "pred_probs = model4.predict_proba(X_selected)[:,1]\n",
    "fpr, tpr, cutoffs = metrics.roc_curve(y, pred_probs)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# Compute area under the ROC curve (AUC)\n",
    "metrics.roc_auc_score(y, pred_probs)\n",
    "\n",
    "# Recursive Feature Elimination and Cross-Validated selection\n",
    "# (using value of C found using cross-validation above)\n",
    "fs = feature_selection.RFECV(lm.LogisticRegression(C=model2.C_[0]),\\\n",
    "                             cv=kf, scoring='roc_auc')\n",
    "fs.fit(X, y)\n",
    "\n",
    "# List selected variables\n",
    "X.columns[fs.support_]\n",
    "\n",
    "# Re-fit model using selected variables only\n",
    "X_selected = X.loc[:,fs.support_]\n",
    "model5 = lm.LogisticRegression(C=model2.C_[0])\n",
    "model5.fit(X_selected, y)\n",
    "\n",
    "# Plot ROC curve\n",
    "pred_probs = model5.predict_proba(X_selected)[:,1]\n",
    "fpr, tpr, cutoffs = metrics.roc_curve(y, pred_probs)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# Compute area under the ROC curve (AUC)\n",
    "metrics.roc_auc_score(y, pred_probs)\n",
    "\n",
    "'''\n",
    "Multiple classes\n",
    "'''\n",
    "\n",
    "# Check the argument `multi_class`:\n",
    "# * 'ovr' means that binary models are estimated for each class\n",
    "# * 'multinomial' means that a single multinomial model is estimated\n",
    "\n",
    "# For example…\n",
    "model6 = lm.LogisticRegression(C=1e50, solver='lbfgs', multi_class='multinomial')\n",
    "model6.fit(X, glioma.Type)\n",
    "\n",
    "model6.classes_\n",
    "np.exp(model6.intercept_)\n",
    "np.exp(model6.coef_)\n",
    "\n",
    "'''\n",
    "Stochastic gradient descent\n",
    "'''\n",
    "\n",
    "# SGD is a very efficient approach to train linear classifiers (including linear\n",
    "# and logistic regression models) on large-scale and/or sparse datasets\n",
    "#\n",
    "# scikit-learn provides:\n",
    "# * `lm.SGDRegressor` for regression problems\n",
    "# * `lm.SGDClassifier` for classification problems\n",
    "#\n",
    "# Both support L1, L2, and Elastic Net regularisation (with parameters 'alpha'\n",
    "# and 'l1_ratio' if using Elastic Net)\n",
    "\n",
    "# SGD is sensitive to scaling of the predictors, so it’s recommended to scale\n",
    "# the data to [0, 1], [-1, 1], or alternatively to standardise it to mean 0 and\n",
    "# variance 1, if there’s no ‘intrinsic scale’ already\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "scaler.mean_\n",
    "scaler.scale_\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# According to the scikit-learn documentation, the following is a good guess for\n",
    "# the number of iterations required to achieve convergence\n",
    "n_iter = np.ceil(10**6 / X.shape[0])\n",
    "\n",
    "# As usual, the regularisation parameter 'alpha' can be tuned using\n",
    "# `grid_search.GridSearchCV`\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=lm.SGDClassifier(loss='log', penalty='l2', n_iter=n_iter),\n",
    "    param_grid={'alpha': 10.0**-np.arange(1, 7)},\n",
    "    scoring='roc_auc',\n",
    "    cv=kf\n",
    ")\n",
    "gs.fit(X_scaled, y)\n",
    "\n",
    "gs.best_estimator_\n",
    "\n",
    "# Before using this model to predict, we'd need to call `scaler.transform` on\n",
    "# the new data\n",
    "\n",
    "# We can also put everything together in a pipeline…\n",
    "\n",
    "sgd_pipeline = Pipeline([\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('sgd', lm.SGDClassifier())\n",
    "])\n",
    "\n",
    "sgd_pipeline.set_params(\n",
    "    sgd__loss='log',\n",
    "    sgd__penalty='l2',\n",
    "    sgd__n_iter=n_iter\n",
    ")\n",
    "\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=sgd_pipeline,\n",
    "    param_grid={'sgd__alpha': 10.0**-np.arange(1, 7)},\n",
    "    scoring='roc_auc',\n",
    "    cv=kf\n",
    ")\n",
    "gs.fit(X, y)\n",
    "\n",
    "gs.best_estimator_\n",
    "\n",
    "# Predictions for new samples can now be obtained directly by calling\n",
    "# `gs.best_estimator_.predict`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ROC analysis\n",
    "'''\n",
    "\n",
    "# Compute predicted probabilities for GBM (y = 1)\n",
    "pred_probs = model1.predict_proba(X)[:,1]\n",
    "\n",
    "# Confirm that model predictions assume a 50% cut-off value\n",
    "assert(np.all((pred_probs >= 0.5) == model1.predict(X)))\n",
    "\n",
    "# Visualise distribution\n",
    "sns.distplot(pred_probs)\n",
    "\n",
    "# Define a set of cut-off values where sensitivity and specificity will be computed\n",
    "cutoffs = np.linspace(0, 1, 1001)\n",
    "\n",
    "# Define a function to compute specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    return cm[0,0] / cm[0,:].sum()\n",
    "\n",
    "# Compute sensitivity and specificity at the cut-off values defined above\n",
    "sensitivities = np.zeros(cutoffs.size)\n",
    "specificities = np.zeros(cutoffs.size)\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    sensitivities[i] = metrics.recall_score(y, pred_probs >= cutoff)\n",
    "    specificities[i] = specificity_score(y, pred_probs >= cutoff)\n",
    "\n",
    "# Plot the ROC curve, i.e. sensitivity versus (1 - specificity)\n",
    "plt.plot(1 - specificities, sensitivities)\n",
    "\n",
    "# Alternatively…\n",
    "# (FPR = 1 - specificity; TPR = sensitivity)\n",
    "fpr, tpr, cutoffs = metrics.roc_curve(y, pred_probs)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# Compute area under the ROC curve (AUC)\n",
    "metrics.roc_auc_score(y, pred_probs)\n",
    "\n",
    "'''\n",
    "Cross-validation\n",
    "'''\n",
    "\n",
    "# Define stratified folds\n",
    "kf = cv.StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "\n",
    "# Compute average classification accuracy across folds\n",
    "accuracies = cv.cross_val_score(lm.LogisticRegression(C=1e50),\\\n",
    "                                X, y, scoring='accuracy', cv=kf)\n",
    "np.mean(accuracies)\n",
    "\n",
    "# Compute average AUC across folds\n",
    "aucs = cv.cross_val_score(lm.LogisticRegression(C=1e50),\\\n",
    "                          X, y, scoring='roc_auc', cv=kf)\n",
    "np.mean(aucs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
